{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd97ed51-d4cf-4dbc-8b33-81285f98ac95",
   "metadata": {},
   "source": [
    "# Create Cross Validation Configuration Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86954550-2133-4188-b484-a9067b5a1878",
   "metadata": {},
   "source": [
    "This notebook creates files containing per-basin time splits for a set of temporal cross-validation model runs. Each cross validaiton split uses ony calendar year (Jan 1 through Dec 31) for testing. The remaining years of data are used for training, with the exception that one year of data is withheld from both training and testing on either side of the test year. This is because the model uses 365 days of data for spinup which makes it necessary to use a 1-year buffer between the train and test periods so that no input data is used for both training and testing. The dates in the train/test splits listed below refer to dates of target (streamflow) data, not dates of input data.\n",
    "\n",
    "Data are available from 1980-2023, inclusive, however ECMWF HRES data and GraphCast data are only available from 2016-2023. These are the two main forecast data sources, so any time period outside of this range will not accurately represent model performance in a real-world setting. We can train on data prior to 2016, but we will not test on data prior to 2016.\n",
    "\n",
    "The train/test cross validation splits are:\n",
    "1) Test: 2016; Train: 1980-2014 & 2018-2023\n",
    "2) Test: 2017; Train: 1980-2015 & 2019-2023\n",
    "3) Test: 2018; Train: 1980-2016 & 2020-2023\n",
    "4) Test: 2019; Train: 1980-2017 & 2021-2023\n",
    "5) Test: 2020; Train: 1980-2018 & 2022-2023\n",
    "6) Test: 2021; Train: 1980-2019 & 2023\n",
    "7) Test: 2022; Train: 1980-2020\n",
    "8) Test: 2023; Train: 1980-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc2f92-8056-437a-a64c-2b30b7b7be45",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c49b1-fdff-4d63-9915-c09d1b47e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import itertools\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34ebf4e5-7a21-4149-93b1-c27995129585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlehydrology.utils.config import Config\n",
    "from googlehydrology.datautils.utils import load_basin_file\n",
    "from googlehydrology.datasetzoo.caravan import load_caravan_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505e014-5263-4d1f-a339-8264a8529236",
   "metadata": {},
   "source": [
    "## User-Defined Variables\n",
    "\n",
    "* `BASE_CONFIG_PATH`: Path to a config file with all model, data, and training settings except basin lists and time periods.\n",
    "* `EXPERIMENT_NAME`: Name of experiment to use for config files, etc.\n",
    "* `BASIN_LIST_DIRECTORY`: Directory where basin lists for this cross-validation experiment are stored. These basin list files must be generated separately.\n",
    "* `CONFIG_FILE_DIRECTORY`: Directory where the config files for this cross-validation experiment are written to by running this notebook. This is the output directory of this notebook and will be overwritten.\n",
    "* `TEST_YEARS`: List of years to use as individual test periods (MultiMet is 2016 - 2023).\n",
    "* `FIRST_DATE`: First date of valid data in the dataset (MultiMet is 1980)\n",
    "* `LAST_DATE`: Last date of valid data in the dataset (MultiMet is 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894db2a0-175c-4a4e-ae0a-3a4d434a163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG_PATH = '/home/gsnearing/ecmwf/configs/template_config.yml'\n",
    "EXPERIMENT_NAME = 'gauged-caravan'\n",
    "CONFIG_FILE_DIRECTORY = f'/home/gsnearing/ecmwf/configs/{EXPERIMENT_NAME}/'\n",
    "BASIN_LIST_DIRECTORY = f'/home/gsnearing/ecmwf/basin_lists/{EXPERIMENT_NAME}/'\n",
    "\n",
    "TEST_YEARS = list(range(2016, 2024))\n",
    "FIRST_DATE = '01/01/1980'\n",
    "LAST_DATE = '31/12/2023'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67096689-feb9-48b7-bc6b-3403a89ef275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the user-supplied path strings into pathlib.Path objects.\n",
    "base_config_path = Path(BASE_CONFIG_PATH)\n",
    "basin_list_dir = Path(BASIN_LIST_DIRECTORY)\n",
    "config_file_dir = Path(CONFIG_FILE_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23278761-ff5d-45c2-9501-a10a3fe1d7e7",
   "metadata": {},
   "source": [
    "## Construct Time Splits\n",
    "Constructs time splits where each year is withheld for testing exactly once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39f06d60-3e5d-469b-b0b7-92689fd1832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = '%d/%m/%Y'\n",
    "splits = {\n",
    "    year: \n",
    "    {\n",
    "        'train': [[FIRST_DATE, f'31/12/{year-2}'], [f'01/01/{year+2}', LAST_DATE]],\n",
    "        'test':  [[f'01/01/{year}', f'31/12/{year}']],\n",
    "    } for year in TEST_YEARS\n",
    "}\n",
    "\n",
    "def _compare_dates_gt(datestr1: str, datestr2: str) -> bool:\n",
    "    return pd.to_datetime(datestr1, dayfirst=True) > pd.to_datetime(datestr2, dayfirst=True)\n",
    "\n",
    "# Remove periods that are outside of the date range.\n",
    "def _filter_date_tuples(data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Filters a nested dictionary to remove date tuples where the first date is later than the second.\n",
    "    \"\"\"\n",
    "    filtered_data = {}\n",
    "    for outer_key, inner_dict in data.items():\n",
    "        filtered_inner_dict = {}\n",
    "        for inner_key, date_tuples in inner_dict.items():\n",
    "            # Use a list comprehension to keep only valid tuples\n",
    "            filtered_tuples = [\n",
    "                (d1, d2) for d1, d2 in date_tuples if not _compare_dates_gt(d1, d2)\n",
    "            ]\n",
    "            filtered_inner_dict[inner_key] = filtered_tuples\n",
    "        filtered_data[outer_key] = filtered_inner_dict\n",
    "    return filtered_data               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82cb62c1-c2cc-42fa-b342-3073f80d6004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2016: {'train': [('01/01/1980', '31/12/2014'), ('01/01/2018', '31/12/2023')],\n",
       "  'test': [('01/01/2016', '31/12/2016')]},\n",
       " 2017: {'train': [('01/01/1980', '31/12/2015'), ('01/01/2019', '31/12/2023')],\n",
       "  'test': [('01/01/2017', '31/12/2017')]},\n",
       " 2018: {'train': [('01/01/1980', '31/12/2016'), ('01/01/2020', '31/12/2023')],\n",
       "  'test': [('01/01/2018', '31/12/2018')]},\n",
       " 2019: {'train': [('01/01/1980', '31/12/2017'), ('01/01/2021', '31/12/2023')],\n",
       "  'test': [('01/01/2019', '31/12/2019')]},\n",
       " 2020: {'train': [('01/01/1980', '31/12/2018'), ('01/01/2022', '31/12/2023')],\n",
       "  'test': [('01/01/2020', '31/12/2020')]},\n",
       " 2021: {'train': [('01/01/1980', '31/12/2019'), ('01/01/2023', '31/12/2023')],\n",
       "  'test': [('01/01/2021', '31/12/2021')]},\n",
       " 2022: {'train': [('01/01/1980', '31/12/2020')],\n",
       "  'test': [('01/01/2022', '31/12/2022')]},\n",
       " 2023: {'train': [('01/01/1980', '31/12/2021')],\n",
       "  'test': [('01/01/2023', '31/12/2023')]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_splits = _filter_date_tuples(splits)\n",
    "time_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc095957-876d-41f7-8878-b838d9ebd94b",
   "metadata": {},
   "source": [
    "## Locate Basin List Files\n",
    "Creates a data structure containing the train/test file pairs in a given basin list directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56b1c7f4-7fca-403b-842a-d21ea392cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _organize_basin_files(basin_list_dir: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Searches a directory for 'prefix_train.txt' and 'prefix_test.txt' files,\n",
    "    and organizes them into a nested dictionary.\n",
    "\n",
    "    Args:\n",
    "        basin_list_dir: A pathlib.Path object representing the directory to search.\n",
    "\n",
    "    Returns:\n",
    "        A nested dictionary with the organized file paths.\n",
    "    \"\"\"\n",
    "    organized_files = {}\n",
    "    \n",
    "    # Updated regex to capture the prefix and type (train/test)\n",
    "    pattern = re.compile(r'(?:(.+)_)?(train|test)\\.txt')\n",
    "\n",
    "    for file_path in basin_list_dir.iterdir():\n",
    "        if file_path.is_file() and file_path.suffix == '.txt':\n",
    "            match = pattern.match(file_path.name)\n",
    "            if match:\n",
    "                prefix = match.group(1)     # e.g., 'basin_a', 'another_basin'\n",
    "                file_type = match.group(2)  # 'train' or 'test'\n",
    "\n",
    "                if prefix not in organized_files:\n",
    "                    organized_files[prefix] = {}\n",
    "                \n",
    "                # Store the Path object\n",
    "                organized_files[prefix][file_type] = file_path\n",
    "                \n",
    "    return organized_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe02abd-a62d-4c3b-86b9-d870f9aa01d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{None: {'test': PosixPath('/home/gsnearing/ecmwf/basin_lists/gauged-caravan/test.txt'),\n",
       "  'train': PosixPath('/home/gsnearing/ecmwf/basin_lists/gauged-caravan/train.txt')}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basin_lists = _organize_basin_files(basin_list_dir)\n",
    "basin_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8fd324-070d-42a0-a739-dccf802f3888",
   "metadata": {},
   "source": [
    "## Create Configs\n",
    "Creates separate config files for each run in the cross-product of basin and time splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a7bc43e-b5c5-4681-98e3-e149028e2ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_all_contents(directory_path: Path):\n",
    "    \"\"\"\n",
    "    Deletes all files and subdirectories within the specified directory.\n",
    "    \"\"\"\n",
    "    if not directory_path.is_dir():\n",
    "        print(f\"Error: {directory_path} is not a directory.\")\n",
    "        return\n",
    "\n",
    "    for item in directory_path.iterdir():\n",
    "        if item.is_dir():\n",
    "            shutil.rmtree(item)\n",
    "        else:\n",
    "            item.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f4635-ded4-41cb-b40b-5defbb2f8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cross_validation_config_files(\n",
    "    base_config_path: Path, \n",
    "    output_dir: Path,\n",
    "    basin_lists: dict[str, dict[str, Path]] | None = None,\n",
    "    time_splits: dict[str, dict[str, list[str]]] | None = None\n",
    "):\n",
    "    \"\"\"Create configs for spatiotemporal cross validation splits.\n",
    "\n",
    "    Optionally supply a set of basins lists and/or a set of time splits, and this\n",
    "    function will create config files that implement a cross product of the basin (space)\n",
    "    and time splits. These config files are stored in `output_directory` and the\n",
    "    `run_scheduler.py` script in this library can be used to run the whole set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    base_config_path : Path\n",
    "        Path to a base config file (.yml)\n",
    "    output_dir : Path \n",
    "        Path to a folder where the generated configs will be stored\n",
    "    basin_splits: dict[str, dict[str, list[str]]] | None\n",
    "    \n",
    "    time_splits: dict[str, dict[str, list[str]]] | None\n",
    "        \n",
    "    \"\"\"\n",
    "    # since the output directory is intended to represent one cross-validation experiment, \n",
    "    # delete any configs that already exist.\n",
    "    if output_dir.is_dir():\n",
    "        delete_all_contents(output_dir)\n",
    "\n",
    "    if not output_dir.is_dir():\n",
    "        output_dir.mkdir(parents=True)\n",
    "\n",
    "    if basin_lists is None and time_splits is None:\n",
    "        raise ValueError('Must supply either a time split or a basin split.')\n",
    "\n",
    "    if basin_lists is None:\n",
    "        basin_lists = {'dummy': []}\n",
    "\n",
    "    if time_splits is None:\n",
    "        time_splits = {'dummy': []}\n",
    "        \n",
    "    # load base config as dictionary\n",
    "    base_config = Config(base_config_path)\n",
    "\n",
    "    # keep a list of all configs generated\n",
    "    all_configs = []\n",
    "\n",
    "    # iterate over each possible combination of basin list and time split\n",
    "    for basin_split in basin_lists:\n",
    "        \n",
    "        # update basin list files\n",
    "        if basin_split != 'dummy':\n",
    "            \n",
    "            update_config = basin_lists[basin_split]\n",
    "            \n",
    "            if 'train' in update_config:\n",
    "                update_config['train_basin_file'] = update_config.pop('train')\n",
    "            if 'validation' in update_config:            \n",
    "                update_config['validation_basin_file'] = update_config.pop('validation')\n",
    "            if 'test' in update_config:\n",
    "                update_config['test_basin_file'] = update_config.pop('test')\n",
    "            if any([bf not in update_config for bf in ['train_basin_file', 'test_basin_file']]):\n",
    "                raise ValueError('Basin list inner dictionary must contain files for train and test. Validation is optional.')\n",
    "            \n",
    "            base_config.update_config(update_config)\n",
    "\n",
    "        for time_split in time_splits:\n",
    "\n",
    "            # update experiment name with split IDs\n",
    "            split_name = f'basin_split_{basin_split}_time_split_{time_split}'\n",
    "            name = f'{EXPERIMENT_NAME}_{split_name}'\n",
    "            base_config.update_config({\"experiment_name\": name})\n",
    "\n",
    "            # update period dates\n",
    "            if time_split != 'dummy':\n",
    "                \n",
    "                update_config = time_splits[time_split]\n",
    "                \n",
    "                if 'train' in update_config:\n",
    "                    update_config['train_start_date'] = [dates[0] for dates in update_config['train']]\n",
    "                    update_config['train_end_date'] = [dates[1] for dates in update_config.pop('train')]\n",
    "                if 'test' in update_config:\n",
    "                    update_config['test_start_date'] = [dates[0] for dates in update_config['test']]\n",
    "                    update_config['test_end_date'] = [dates[1] for dates in update_config.pop('test')]                 \n",
    "                if any([tp not in update_config for tp in ['train_start_date', 'train_end_date', 'test_start_date', 'test_end_date']]):\n",
    "                    raise ValueError('Time split inner dictionary must contain start and end dates for train and test. Validation is optional.')\n",
    "      \n",
    "                if 'validation' in update_config:            \n",
    "                    update_config['validation_start_date'] = [dates[0] for dates in update_config['validation']]\n",
    "                    update_config['validation_end_date'] = [dates[1] for dates in update_config.pop('validation')]\n",
    "                elif 'validation_start_date' not in update_config:\n",
    "                    update_config['validation_start_date'] = update_config['test_start_date']\n",
    "                    update_config['validation_end_date'] = update_config['test_end_date']\n",
    "                \n",
    "                base_config.update_config(update_config)\n",
    "\n",
    "            # write new config to output directory\n",
    "            base_config.dump_config(output_dir, f\"{name}.yml\")\n",
    "            all_configs.append(output_dir/f\"{name}.yml\")\n",
    "        \n",
    "    return all_configs\n",
    "\n",
    "    print(f\"Finished. Configs are stored in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "337274f0-884c-443f-a3ed-041ecf27a732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 configs were generated.\n"
     ]
    }
   ],
   "source": [
    "all_configs = create_cross_validation_config_files(\n",
    "    base_config_path=base_config_path, \n",
    "    output_dir=config_file_dir,\n",
    "    basin_lists=basin_lists,\n",
    "    time_splits=time_splits\n",
    ")\n",
    "print(f'{len(all_configs)} configs were generated.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9731d28-9da0-4f8d-a3c6-d2a011ba76c8",
   "metadata": {},
   "source": [
    "## Remove Zero-Variance Static Features\n",
    "Static features that are constant across all basins in the train set will have 0-variance scalers and cause NaN's in training. It is therefore necessary to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4029e61a-5f25-4c48-9e5a-e0ddf6f10656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload all attributes.\n",
    "cfg = Config(all_configs[0])\n",
    "attributes_df = load_caravan_attributes(data_dir=Path(cfg.statics_data_dir))[cfg.static_attributes].to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb642d42-75be-44a0-942c-c6e0f5b6faf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70bf6fca5d443ffb8fe736bb8c49dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find any attribute that is unsafe in any split.\n",
    "unsafe_attributes = []\n",
    "for config_path in tqdm(all_configs):\n",
    "    cfg = Config(config_path)\n",
    "    basins = load_basin_file(cfg.train_basin_file)\n",
    "    basin_attributes_df = attributes_df.loc[basins]\n",
    "    variances = basin_attributes_df.var()    \n",
    "    unsafe_attributes.extend(variances[variances == 0].index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14fd69ef-3d7c-415e-b838-92a0edc75f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 86 attributes total.\n",
      "There are 85 attributes with non-zero variance.\n",
      "Attributes with at least one nonzero variance: {'glc_pc_s05'}\n"
     ]
    }
   ],
   "source": [
    "# Combine unsafe attributes and create list of safe attributes.\n",
    "unsafe_attributes = set(unsafe_attributes)\n",
    "safe_attributes = set(attributes_df.columns) - unsafe_attributes\n",
    "print(f'There are {len(cfg.static_attributes)} attributes total.')\n",
    "print(f'There are {len(safe_attributes)} attributes with non-zero variance.')\n",
    "print('Attributes with at least one nonzero variance:', unsafe_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8701461-1e63-47ca-81c6-fb07cec3dc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190296523ee84fc1887fd3eb5cbbe2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Overwrite config files with new static attributes list.\n",
    "for config_path in tqdm(all_configs):\n",
    "    cfg = Config(config_path)\n",
    "    config_path.unlink()\n",
    "    cfg.update_config({'static_attributes': safe_attributes})\n",
    "    cfg.dump_config(folder=config_path.parent, filename=config_path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dbcd1f-c960-49c6-95b0-d27c33cd03cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
